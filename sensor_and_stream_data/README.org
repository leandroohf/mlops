
* Phase 1: Project Overview: MVP for Sensor Data Ingestion


  | what                 | frequency          | process duration | scope             | Comment             |
  |----------------------+--------------------+------------------+-------------------+---------------------|
  | data ingestion       | close to real time | < 5 minutes      | simple and sparse |                     |
  | features engineering |                    |                  |                   | not implemented yet |
  | ml training          |                    |                  |                   | out of scope        |
  | ml predictions       |                    |                  |                   | out of scope        |

   * projects assets (BOM: Bil of materials)
   | assets           | qtd | comm                           |
   |------------------+-----+--------------------------------|
   | pubsub           |   1 | 1 topic                        |
   | dataset          |   1 |                                |
   | tables           |   2 | raw and features               |
   | storage service  |   1 | centralize but dufferent paths |
   | cloud function   |   1 |                                |
   | service accounts |   1 |                                |

    This project is a minimal learning prototype (MVP) that simulates inject
    sparse data from sensors using Google Cloud services. The goal is to:

    1. Simulate sparse sensor data (e.g., bike station pickups and drop-offs).
    2. Send sensor events to a Google Cloud Pub/Sub topic.
    3. Use a Cloud Function to consume Pub/Sub messages and write raw events
       into a BigQuery table.

    This is a first step toward building a full streaming architecture. Later
    phases may include additional components like a feature computation layer
    (via Dataflow or other tools), but this MVP focuses on establishing a
    reliable ingestion pipeline.

** Why Cloud Function?

  + **Latency is not a concern** at this stage, as sensor data is sparse and
    events do not arrive in high bursts.
  + Cloud Functions offer a **low-maintenance, serverless option** to bridge
    Pub/Sub and BigQuery quickly.
  + This project is intentionally designed as an MVP. Later, we plan to switch
    to Cloud Dataflow for more scalable and complex processing.

** BigQuery Tables schema

  Two tables are created for this MVP:

  1. **Raw Events Table** (`raw_events`): Stores the exact sensor events coming
     from Pub/Sub.

     * Optimized (Simple) for fast inserts; avoids redundancy

        | event_id | station_id | ts_iso               | kind    | delta |
        |----------+------------+----------------------+---------+-------|
        | e-90001  | A_101      | 2025-10-18T16:24:11Z | pickup  |     1 |
        | e-90002  | A_101      | 2025-10-18T16:24:44Z | dropoff |     1 |
        | e-90003  | A_205      | 2025-10-18T16:25:02Z | pickup  |     1 |

  2. **Feature Events Table** (`station_features`): Reserved for future use by a
     feature computation function, not used in this MVP.

     * Simple to add remove features but require extra pivoting for training
       (Not the scope of this project)

        | station_id | feature_name     | window_len | value | as_of               |
        |------------+------------------+------------+-------+---------------------|
        | A_101      | pickup_count     | 2h         |     8 | 2025-11-16 02:15:00 |
        | A_101      | avg_delta_pickup | 24h        |   3.2 | 2025-11-16 02:15:00 |
        | A_101      | is_peak_station  | all_time   |   1.0 | 2025-11-16 02:15:00 |

** Architecture Diagram (Text Description)

   #+BEGIN_SRC
   Sensor Simulator --> Pub/Sub Topic --> Cloud Function --> BigQuery (raw_events)
   #+END_SRC

** Setup infrastructure

  * Env variables

    #+begin_src sh
      # NOTE: Environment setup
      export PROJECT_ID="mlops-project-id"
      export REGION="us-central1"
      export SA="bike-share-job@${PROJECT_ID}.iam.gserviceaccount.com"
      export BUCKET="gs://${PROJECT_ID}-bike-share-project-bucket"

      export PUBSUB_TOPIC="projects/${PROJECT_ID}/topics/pubsub-sensor"
      export SUBSCRIPTION="projects/${PROJECT_ID}/subscriptions/sensor-subscription"

      export RAW_EVENT_TABLE="${PROJECT_ID}.bike_streaming.raw_events"


      # NOTE: for phase 2 n 3
      export FEATURES_TABLE="${PROJECT_ID}.bike_streaming.station_features"

      # NOTE: Dataflow job name
      export INJECT_SENSOR_DATA_JOB_NAME="bike-share-pubsub-to-raw-events-table" # NOTE: Unique among active jobs in the same region
      export FEATURE_ENGINEERING_JOB_NAME="bike-share-raw-events-table-to-features-table"

      export REGISTRY="bike-share"
      export IMAGE="${REGION}-docker.pkg.dev/${PROJECT_ID}/${REGISTRY}/df-launcher:latest"


      # NOTE: show me dont tell me
      echo "PROJECT_ID=${PROJECT_ID}"
      echo "REGION=${REGION}"
      echo "SA=${SA}"
      echo "BUCKET=${BUCKET}"
      echo "PUBSUB_TOPIC=${PUBSUB_TOPIC}"
      echo "SUBSCRIPTION=${SUBSCRIPTION}"
      echo "RAW_EVENT_TABLE=${RAW_EVENT_TABLE}"
      echo "FEATURES_TABLE=${FEATURES_TABLE}"
      echo "INJECT_SENSOR_DATA_JOB_NAME=${INJECT_SENSOR_DATA_JOB_NAME}"
      echo "FEATURE_ENGINEERING_JOB_NAME=${FEATURE_ENGINEERING_JOB_NAME}"
      echo "GOOGLE_APPLICATION_CREDENTIAL=${GOOGLE_APPLICATION_CREDENTIAL}"
      echo "REGISTRY=${REGISTRY}"
      echo "IMAGE=${IMAGE}"
    #+end_src

  * Creating table

    #+begin_src sh
      # NOTE: raw event table
      # PS: wee need double quotes for bash expansion
      bq query --use_legacy_sql=false "
      CREATE TABLE IF NOT EXISTS \`${RAW_EVENT_TABLE}\` (
        event_id STRING,
        station_id STRING,
        ts_iso TIMESTAMP,
        kind STRING,
        delta INT64,
        event_date  DATE
      )"

      # NOTE: checking datasets tables were created
      # list all datasets
      bq ls --project_id=$PROJECT_ID

      # list all tables in datasets
      bq ls $PRJECT_ID:bike_streaming
      tableId        Type    Labels   Time Partitioning   Clustered Fields
      ------------------ ------- -------- ------------------- ------------------
      raw_events         TABLE
      station_features   TABLE

      # delete table
      bq rm -f -t $PRJECT_ID:bike_streaming.events

    #+end_src

  * Create the pubusub and the topic

    #+begin_src sh
      # NOTE: enabling
      gcloud services enable pubsub.googleapis.com --project=$PROJECT_ID

      # NOTE: creating the topic
      gcloud pubsub topics create $PUBSUB_TOPIC --project="$PROJECT_ID"

      # NOTE: listing topics
      gcloud pubsub topics list --project="$PROJECT_ID"

      # NOTE: guaranteing publish permission to SA. Assuming SERVICE ACCOUNT is already created before
      gcloud pubsub topics add-iam-policy-binding $PUBSUB_TOPIC \
             --member="serviceAccount:$SA" \
             --role="roles/pubsub.publisher" \
             --project="$PROJECT_ID"


      # NOTE: subscribe SA to topic PUBSUB_TOPIC (he can publish to the right topic)
      # subscription is for consumers
      gcloud pubsub subscriptions create $SUBSCRIPTION \
             --topic=$PUBSUB_TOPIC \
             --project="$PROJECT_ID"


      # NOTE: listing and deleting subscriptions
      gcloud pubsub subscriptions list --project="$PROJECT_ID"

      gcloud pubsub subscriptions delete $SUBSCRIPTION --project="$PROJECT_ID"


      # NOTE: make sure the script sensor.py has the right permission
      # 1) create the key for service account
      #    This allow you run locally using the sa
      gcloud iam service-accounts keys create ~/bike-share-job-key.json \
             --iam-account=$SA

      # 2) set env to use the the key
      export GOOGLE_APPLICATION_CREDENTIALS=~/bike-share-job-key.json

      # NOTE: run and test up to now
      python scripts/sensors.py

      # WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
      # E0000 00:00:1761162298.761872 63406587 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.

      # Published: {'event_id': 'e-90001', 'station_id': 'A_101', 'ts_iso': '2025-10-22T19:44:58+00:00', 'kind': 'dropoff', 'delta': 1}
      # Published: {'event_id': 'e-90002', 'station_id': 'A_101', 'ts_iso': '2025-10-22T19:45:00+00:00', 'kind': 'pickup', 'delta': 1}
      # Published: {'event_id': 'e-90003', 'station_id': 'A_205', 'ts_iso': '2025-10-22T19:45:00+00:00', 'kind': 'dropoff', 'delta': 1}
      # Published: {'event_id': 'e-90004', 'station_id': 'A_310', 'ts_iso': '2025-10-22T19:45:01+00:00', 'kind': 'pickup', 'delta': 1}
      # Published: {'event_id': 'e-90005', 'station_id': 'A_205', 'ts_iso': '2025-10-22T19:45:02+00:00', 'kind': 'dropoff', 'delta': 1}


      # NOTE: tetsing with gcloud before
      # pull future messages
      gcloud pubsub subscriptions pull $SUBSCRIPTION \
             --limit=15 \
             --auto-ack \
             --project="$PROJECT_ID"

      # ┌──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┬───────────────────┬──────────────┬────────────┬──────────────────┬────────────┐
      # │                                                         DATA                                                         │     MESSAGE_ID    │ ORDERING_KEY │ ATTRIBUTES │ DELIVERY_ATTEMPT │ ACK_STATUS │
      # ├──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┼───────────────────┼──────────────┼────────────┼──────────────────┼────────────┤
      # │ {"event_id": "e-90001", "station_id": "A_205", "ts_iso": "2025-10-19T21:02:06+00:00", "kind": "dropoff", "delta": 1} │ 16637501616397770 │              │            │                  │ SUCCESS    │
      # │ {"event_id": "e-90003", "station_id": "A_101", "ts_iso": "2025-10-19T21:02:09+00:00", "kind": "dropoff", "delta": 1} │ 16637231472208991 │              │            │                  │ SUCCESS    │
      # │ {"event_id": "e-90002", "station_id": "A_310", "ts_iso": "2025-10-19T21:02:08+00:00", "kind": "dropoff", "delta": 1} │ 16638886842299705 │              │            │                  │ SUCCESS    │
      # └──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┴───────────────────┴──────────────┴────────────┴──────────────────┴────────────┘
    #+end_src

  * Deploying the Cloud Function

    #+begin_src sh
      # NOTE: deploy cloud function
      # the command will ask if you want to enable all necessaries sevices and APIs
      # the source code is in cloud_functions/raw_ingest_to_bq/
      gcloud functions deploy pubsub_to_bq \
              --runtime python310 \
              --trigger-topic $PUBSUB_TOPIC \
              --entry-point pubsub_to_bq \
              --region us-central1 \
              --source cloud_functions/raw_ingest_to_bq \
              --memory 128Mi \
              --timeout 120s \
              --allow-unauthenticated \
              --set-env-vars=RAW_EVENT_TABLE=$RAW_EVENT_TABLE \
              --no-gen2  # NOTE: needs gen1 because pubsub does not expose http port (LLM)

      # NOTE: checking
      gcloud functions list --project=$PROJECT_ID

      # delete
      gcloud functions delete pubsub_to_bq \
             --region us-central1 \
             --gen2 \
             -q
    #+end_src

** How to run and test


   #+begin_src sh
     # NOTE: Run the sensors
     python scripts/sensors.py

     # NOTE: checking if the cloud function were executed
     gcloud functions logs read pubsub_to_bq \
            --region=us-central1 \
            --limit=50 \
            --project=$PROJECT_ID

     # NOTE: show the table on bigquery
     bq query --use_legacy_sql=false "
     SELECT * FROM \`${RAW_EVENT_TABLE}\`
     LIMIT 5"
     # +----------+------------+---------------------+---------+-------+
     # | event_id | station_id |       ts_iso        |  kind   | delta |
     # +----------+------------+---------------------+---------+-------+
     # | e-90001  | A_205      | 2025-10-26 03:13:21 | pickup  |     1 |
     # | e-90002  | A_205      | 2025-10-26 03:13:23 | pickup  |     1 |
     # | e-90006  | A_101      | 2025-10-26 03:13:32 | pickup  |     1 |
     # | e-90003  | A_101      | 2025-10-26 03:13:25 | dropoff |     1 |
     # | e-90007  | A_310      | 2025-10-26 03:13:34 | pickup  |     1 |
     # +----------+------------+---------------------+---------+-------+
   #+end_src

* Phase 2: Scalable Ingestion Pipeline with Cloud Dataflow


  | what                 | frequency          | process duration | scope                                    | changes                                  | comment                                |
  |----------------------+--------------------+------------------+------------------------------------------+------------------------------------------+----------------------------------------|
  | data ingestion       | close to real time | < 5 minutes      | simple with high volume                  | switched from Cloud Function -> Dataflow | data ingested as soon as created       |
  | features engineering | every 1 hour       | < 15 minutes     | simple. **min window lentgh 30 minutes** | new (not present in Phase 1)             | cloud function                         |
  | ml training          | every 2 hour       | < 30 minutes     | one model for all data                   | still out of scope                       |                                        |
  | ml predictions       | every 1 hour       | < 10 minutes     | synced with feature engineering          | still out of scope                       | multiple runs due to changing features |
  * NOTEs:
     * ml model training and prediction is out of the scope of this project

   * projects assets (BOM: Bil of materials)
   | assets                         | qtd | comm                                          |
   |--------------------------------+-----+-----------------------------------------------|
   | pubsub                         |   1 | 1 topic                                       |
   | dataset                        |   1 |                                               |
   | tables                         |   2 | raw and features                              |
   | storage service                |   1 | centralize but dufferent paths                |
   | cloud scheduler                |   1 | optional. wont be implemented in this project |
   | dataflow jobs  stream pipeline |   1 | inject sensor dtaa i raw events table         |
   | cloud function                 |   1 |                                               |
   | service accounts               |   1 |                                               |


 This is the second phase of our ingestion pipeline. In this version:

    * We simulate a continuous, high-frequency sensor stream.
    * We replace the Cloud Function with Cloud Dataflow (Apache Beam).
    * This enables scalable data injection and fault-tolerant processing of
      Pub/Sub messages into BigQuery.
    * We also introduce a feature computation layer to aggregate raw events into
      useful features. In this phase it is not necessary to scale because the
      features are simple and not required often.

    #+begin_src markdown
      ```mermaid
      flowchart TD
        A[Sensor Data] --> B[Pub/Sub Topic]
        B --> C[Dataflow Job #1<br>Raw Events → BigQuery: raw_events]
        C --> D[Dataflow Job #2 or Batch SQL<br>Aggregations → station_features]
      ```
    #+end_src

    * The features are simple and wont take longer. Also they are required every
      hour. We use a scheduler and cloud function to compute the features

** TOR: Dataflow Jobs (Apache Beam)

   PS: Shamelessly stolen from:
   * https://medium.com/@ofelipefernandez/project-processing-streaming-data-with-pub-sub-and-dataflow-631e6082c94d
   * See: ../apache_beam_cheet_sheet.md

   Apache Beam is a unified programming model for building both batch and
   streaming data processing pipelines. It allows you to write code once and run
   it on various distributed processing backends, like Apache Flink, Apache
   Spark, and Google Cloud Dataflow.

   Apache Beam syntax is similar to dplyr in R:

   #+begin_src python
     # NOTE: data is sent to the next step via pipes
     data | "Step name" >> beam.Map(my_function) \
          | "Squared step" >> beam.Map(lambda x: x ** 2)
   #+end_src

  * Status:
    - **Running**: listening to Pub/Sub and executing the job
    - **Cancelled**: the job is stopped (no longer listening or processing)
    - **Drained**: same as cancelled, but ensures no data loss

  * Drain a Job (graceful shutdown)
    - The pipeline stops accepting new input (e.g., from Pub/Sub).
    - It continues processing any buffered or in-progress data (e.g., windowed
      aggregations).
    - Once all pending work is completed, the job transitions to the **Drained** state.

    - Useful for:
      - Shutting down the pipeline safely without losing data
      - Performing upgrades or maintenance with a clean stop
      - Switching to a new job without cancelling mid-stream

  #+begin_src markdown
    ```mermaid
    flowchart LR
      START[STOPPED] --> RUNNING
      RUNNING --> DRAINING
      DRAINING --> DRAINED
      RUNNING --> CANCELLED
      RUNNING --> FAILED
    ```
  #+end_src


  * stream of data: PCollections
    * sequency of dictionaries (not list) that can be processed sequential or in
      parallel (distributed)

      #+begin_src python
        # NOTE: apache beam data process mental model
        for row in PCollection:
            process(row)  # row is a dict
      #+end_src

** Setup the cloud dataflow (Apache beam)

    * env vars: We gonna use the same as before plus:

      #+begin_src sh
        # NOTE: Dataflow job name
        INJECT_SENSOR_DATA_JOB_NAME="bike-share-raw-events" # NOTE: Unique among active jobs in the same region
      #+end_src

    * Cloud dataflow


      1. **Option 1**: start using dtaflow templates (easy n simple but hard to
         personalize)

         #+begin_src sh
           # NOTE: enable and create apache bean cloud dataflow
           gcloud services enable \
                  dataflow.googleapis.com --project="$PROJECT_ID"

           # NOTE: give permissions
           # 1) Allows a running Dataflow job to access resources
           gcloud projects add-iam-policy-binding "$PROJECT_ID" \
                  --member="serviceAccount:$SA" \
                  --role="roles/dataflow.worker"


           # 2) creating, updating and launching jobs
           gcloud projects add-iam-policy-binding $PROJECT_ID \
                  --member="serviceAccount:$SA" \
                  --role="roles/dataflow.developer"


           # NOTE: delete the cloud function. We dont want trigger them in this phase
           gcloud functions delete pubsub_to_bq \
                  --region us-central1 \
                  --gen2 \
                  -q

           # NOTE: Create run dataflow TEMPLATE job (GOOD FOR QUICK TESTING. START WITH THIS THEN SWITCH TO APACHE BEAM CODE)
           gcloud dataflow jobs run "$INJECT_SENSOR_DATA_JOB_NAME" \
                  --gcs-location "gs://dataflow-templates-$REGION/latest/PubSub_Subscription_to_BigQuery" \
                  --region "$REGION" \
                  --staging-location "$BUCKET/staging" \
                  --service-account-email "$SA" \
                  --worker-machine-type="n1-standard-1" \
                  --num-workers=1 \
                  --max-workers=1 \
                  --parameters="inputSubscription=$SUBSCRIPTION,outputTableSpec=$RAW_EVENT_TABLE"

           # NOTE: inspect the jobs
           gcloud dataflow jobs list --region "$REGION"

           JOB_ID=2025-11-15_16_52_15-3602385465536419454
           gcloud dataflow jobs describe "$JOB_ID" --region="$REGION"

           # NOTE: Give Dataflow’s service agent access to BigQuery:
           gcloud projects add-iam-policy-binding $PROJECT_ID \
                    --member="serviceAccount:$SA" \
                    --role="roles/bigquery.dataEditor" \
                    --project="$PROJECT_ID"

           # NOTE: run sensor script
           python scripts/sensors.py

           # NOTE: check tables contents
           bq query --use_legacy_sql=false "SELECT * FROM \`$PROJECT_ID.bike_streaming.raw_events\` LIMIT 10;"


           # NOTE: how to stop job or drain
           gcloud dataflow jobs cancel 2025-11-15_17_21_13-13963551468804225520 --region=us-central1
           gcloud dataflow jobs drain  --region "$REGION" --job="$INJECT_SENSOR_DATA_JOB_NAME"  # NOTE: drain = graceful stop

           # NOTE: for debug (but better use the webui)
           # NOTE: seeing logs of an execution
           gcloud logging read \
                  "resource.type=dataflow_step AND resource.labels.job_name=bike-share-raw-events AND severity>=ERROR" \
                  --limit=500 \
                  --project="$PROJECT_ID"
         #+end_src

      2. **Option 2**: using apache beam python interface

         #+begin_src sh
           # NOTE: submitt apache beam job to dataflow
           python scripts/create_apache_beam_stream_pubsub_to_raw_events.py &

           # NOTE: inspect and verify jobs
           gcloud dataflow jobs list --region "$REGION"

           # NOTE: test
           python scripts/sensors.py

           bq query --use_legacy_sql=false "SELECT * FROM \`$RAW_EVENT_TABLE\` ORDER BY ts_iso desc  LIMIT 10;"
         #+end_src

** Features engineering

    The features table change frequent because the ML models change often the
    features are used as input.

   * Features: Count Pickups/Dropoffs in Last 2 Hours & Store

     1. Create the features table

        #+begin_src sh
          # NOTE: tall feature table
          bq query --use_legacy_sql=false "
                CREATE TABLE IF NOT EXISTS \`${FEATURES_TABLE}\` (
                  station_id STRING,
                  feature_name STRING,
                  window_len STRING,
                  value FLOAT64,
                  as_of TIMESTAMP
                )"
          station_id | feature_name       | window_len | value | as_of
          -----------|--------------------|------------|-------|---------------------
          A_101      | pickup_count       | 2h         | 8     | 2025-11-16 02:15:00
          A_101      | avg_delta_pickup   | 24h        | 3.2   | 2025-11-16 02:15:00
          A_101      | is_peak_station    | all_time   | 1.0   | 2025-11-16 02:15:00

          # NOTE: list all tables
          bq ls $PRJECT_ID:bike_streaming
        #+end_src

        * Pros:
          * Schema is stable—never changes, no matter how many features or
            windows.
          * Supports tracking multiple versions or window lengths for same
            feature.
          * Easily query features using filters like:

              #+begin_src sql
                SELECT *
                FROM feature_events
                WHERE station_id = 'A_101' AND window_len = '2h'
              #+end_src

        * Crons:

          * Harder to train directly with ML frameworks—requires pivoting

            #+begin_src sql
              -- NOTE: Pivot for training
              SELECT *
              FROM (
                   SELECT station_id, feature_name, value
                   FROM feature_events
                   WHERE window_len = '2h' AND as_of = 'latest'
              )

              PIVOT (
                    MAX(value) FOR feature_name IN ('pickup_count', 'dropoff_count')
              )
            #+end_src

          * Might need more joins during training/inference
          * More costly

              #+begin_src sql
                SELECT
                  station_id,
                  COUNTIF(kind = 'pickup') AS pickup_count,
                  COUNTIF(kind = 'dropoff') AS dropoff_count,
                  CURRENT_TIMESTAMP() AS computed_at

                FROM
                  `mlops-project-abacabb.bike_streaming.raw_events`

                WHERE
                  ts_iso >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 2 HOUR)

                GROUP BY station_id;
              #+end_src

    * deploying the cloud function

      #+begin_src sh
        # NOTE: deploy cloud function to perform features engineering (gen 2)
        # source code located in cloud_functions/update_station_features/
        gcloud functions deploy update_station_features \
               --runtime python310 \
               --trigger-http \
               --entry-point compute_and_upsert_features \
               --region us-central1 \
               --source cloud_functions/update_station_features \
               --memory 128Mi \
               --timeout 120s \
               --allow-unauthenticated \
               --set-env-vars=RAW_EVENT_TABLE=$RAW_EVENT_TABLE,FEATURES_TABLE=$FEATURES_TABLE

        # NOTE: checking
        gcloud functions list --project=$PROJECT_ID
        gcloud functions describe update_station_features --region=us-central1 --format="value(httpsTrigger.url)"

        # NOTE: trigger for testing
        gcloud functions call update_station_features --region us-central1

        # NOTE inspect logs
        gcloud functions logs read update_station_features --region=us-central1 --limit=50

        bq query --use_legacy_sql=false "SELECT * FROM \`$FEATURES_TABLE\` ORDER BY as_of DESC;"
      #+end_src

    * deploying the cloud scheduler

       #+begin_src sh
         # NOTE: get cloud funct http trigger
         gcloud functions list --project=$PROJECT_ID
         gcloud functions describe update_station_features \
                --region=${REGION} \
                --format="value(serviceConfig.uri)"

         URI="https://update-station-features-cc5zus76ja-uc.a.run.app"
         # NOTE: schedule every 30 minutes
         gcloud scheduler jobs create http update-station-features-job \
                --schedule="*/30 * * * *" \
                --http-method=POST \
                --uri=${URI} \
                --message-body="{}" \
                --time-zone="America/Los_Angeles" \
                --location=${REGION}

         # NOTE: inspect
         gcloud scheduler jobs list --location=$REGION

         gcloud scheduler jobs describe update-station-features-job --location=$REGION

         # NOTE: test solution

         # 0) make sure dataflow is running
         gcloud dataflow jobs list --region "$REGION"
         # 1) inspect the state of features table
         bq query --use_legacy_sql=false "SELECT * FROM \`$FEATURES_TABLE\` ORDER BY as_of DESC;"
         # 2) run sensor
         python scripts/sensors.py
         # 3): manually trigger the job
         gcloud scheduler jobs run update-station-features-job --location=${REGION}
         # 4) inspect the state of features table
         bq query --use_legacy_sql=false "SELECT * FROM \`$FEATURES_TABLE\` ORDER BY as_of DESC;"
       #+end_src

* Phase 3: Scalable Pipeline

   | what                 | frequency          | process duration        | scope                                     | changes                                       | description                                |
   |----------------------+--------------------+-------------------------+-------------------------------------------+-----------------------------------------------+--------------------------------------------|
   | data ingestion       | close to real time | < 5 minutes             | simple with high volume                   | same ingestion pattern, higher global scale   | data are ingested as soon as it is created |
   | features engineering | every 25 minutes   | < 1h                    | complex. **min window lentgh 30 minutes** | upgraded from simple -> complex + higher freq |                                            |
   | ml training          | every 2 hour       | < 1h                    | one model per region                      | switched from 1 global model -> per region    | 1 model per city/region                    |
   | ml predictions       | every 25 minutes   | < 20 minutes per region | per region for next 3 hours               | higher frequency + regionalized predictions   |                                            |
   * NOTEs:
     * regions can be: 1 city (moderate size), a group of city (for small cities)
       or neighborhoods for large cities
     * For simplicty we gonna treat a region as a city
     * ml model training and prediction is out of the scope of this project

   * projects assets (BOM: Bil of materials)
   | assets                               | qtd | comm                                          |
   |--------------------------------------+-----+-----------------------------------------------|
   | pubsub                               |   1 | 1 topic                                       |
   | dataset                              |   1 |                                               |
   | tables                               |   2 | raw and features                              |
   | storage service                      |   1 | centralize but dufferent paths                |
   | dataflow jobs  stream/batch pipeline |   2 | inject and feature engineering                |
   | cloud scheduler                      |   1 | optional. wont be implemented in this project |
   | service accounts                     |   1 |                                               |

  In Phase 3 we have the requirments that project grew up to different
  locations, cities and countries and the feature engineering are more complex
  and required to higher frequency than before.

   For most of the cities parttioning the raw_events table and features tables
   by day will be enough, but for large cities like New York, Sao Paulo and
   London the partitioning by date wont be enough. Because of this cases we will
   cluster the tables by station_id and cities.

   Here is an summary of the requirements:

   * The number of countries,cities, station are higher
   * The sensor data are high volume (ingestion needs to scale)
   * The feature get complex and need to be computed more often (features
     engineering needs to scale n dbs be partitioned anc clustered
     * partition by day
     * clustered by station_id and cities

    #+begin_src markdown
      ```mermaid
      flowchart TD

        %% Streaming Ingestion
        subgraph "Streaming Ingestion (Real-time)"
          UserClient["Sensor Script (sensors.py)"] --> PubSub["> Pub/Sub Topic\n(pubsub-sensor)"]
          PubSub --> StreamJob["Dataflow Job (Streaming)\ncreate_apache_beam_stream_pubsub_to_raw_events.py"]
          StreamJob --> BQRaw["BigQuery Table\nbike_streaming.raw_events"]
        end

        %% Scheduled Batch Feature Engineering
        subgraph "Batch Feature Engineering (Scheduled)"
          CloudScheduler["Cloud Scheduler\n(every 15–60 min)"] --> TriggerBatch["Trigger Batch Job"]
          TriggerBatch --> BatchJob["Dataflow Job (Batch)\ncreate_apache_beam_batch_raw_events_to_features.py"]
          BatchJob --> BQFeatures["BigQuery Table\nbike_streaming.station_features"]
          BatchJob --> GCSDebug2["GCS Bucket\nfeature-debug/"]
        end

        %% Styling
        classDef bq fill:#E4F1FB,stroke:#333,stroke-width:1px;
        classDef gcs fill:#FFF4E6,stroke:#d18,stroke-width:1px;
        classDef df fill:#DFFFD6,stroke:#333,stroke-width:1px;
        classDef pubsub fill:#FFF9C4,stroke:#999,stroke-width:1px;
        classDef scheduler fill:#F0E5FF,stroke:#845EC2,stroke-width:1px;

        class PubSub pubsub;
        class StreamJob,BatchJob df;
        class BQRaw,BQFeatures bq;
        class GCSDebug2,GCS gcs;
        class CloudScheduler,scheduler scheduler;
      ```
    #+end_src

   GOAL:
     * db should be scalable
     * features engineering should be scalable (dataflow as batch)
       * replace cloud function by dataflow batch


     * Create the raw_events and station_features table
       * we decide to partitioning raw events table for quick ingestion and also
         provide better reads
       * we decide to partiotining and cluster features table for good ingestion
         but better reads.

        #+begin_src sh
          # NOTE: list all tables
          bq ls $PRJECT_ID:bike_streaming

          # NOTE: delete old tables
          bq rm -f "${RAW_EVENT_TABLE}"
          bq rm -f "${FEATURES_TABLE}"

          # NOTE:
          # NOTE: raw event table
          # PS: wee need double quotes for bash expansion
          bq query --use_legacy_sql=false "
          CREATE TABLE IF NOT EXISTS \`${RAW_EVENT_TABLE}\` (
            event_id STRING,
            station_id STRING,
            ts_iso TIMESTAMP,
            kind STRING,
            delta INT64,
            event_date DATE
          )
          PARTITION BY event_date
          "

          # NOTE: features table
          # PS: wee need double quotes for bash expansion
          bq query --use_legacy_sql=false "
          CREATE TABLE IF NOT EXISTS \`${FEATURES_TABLE}\` (
            city_id STRING,
            station_id STRING,
            feature_name STRING,
            window_len STRING,
            value FLOAT64,
            as_of TIMESTAMP,
            event_date DATE
          )
          PARTITION BY event_date
          CLUSTER BY city_id, station_id; -- NOTE: like sorting by city first, then by station_id within each city
          "
        #+end_src

     * Features engineering

       + Initial version: Cloud Run Job triggered by Cloud Scheduler (We won't
         implement Cloud Run Job in this tutorial)
         - Suitable for BigQuery-based features
         - Handles up to ~10M rows per run
         - Simple to deploy and observe

       + Planned migration path:
         - Switch to Apache Beam when:
           - Feature logic becomes complex (Python, joins, custom rules)
           - Data scale increases beyond 10–50M rows
         - Use Dataflow (Apache Beam) as scalable backend

       + Creating apache beam batch pipeline

         + as opposed to stream pipeline that is always running, this one must
           be triggered manually or by cloud scheduler

          #+begin_src sh
            # NOTE: run the batch apache beam batch pipeline
            # TODO: Parei aqui.
            # preciso adicionar argumentos usando typer
            # default current time but the args is useful for re processing
            # NOTE: local run
            python scripts/run_apache_beam_batch_raw_events_to_features.py


            # NOTE: run google cloud
            python scripts/run_apache_beam_batch_raw_events_to_features.py --runner="DataflowRunner" --run-timestamp=$(date -u +"%Y-%m-%dT%H:%M:%SZ")
          #+end_src

     * Checking infrastructure

         #+begin_src sh
           # NOTE: show me dont tell me
           echo "PROJECT_ID=${PROJECT_ID}"
           echo "REGION=${REGION}"
           echo "SA=${SA}"
           echo "BUCKET=${BUCKET}"
           echo "PUBSUB_TOPIC=${PUBSUB_TOPIC}"
           echo "SUBSCRIPTION=${SUBSCRIPTION}"
           echo "RAW_EVENT_TABLE=${RAW_EVENT_TABLE}"
           echo "FEATURES_TABLE=${FEATURES_TABLE}"
           echo "INJECT_SENSOR_DATA_JOB_NAME=${INJECT_SENSOR_DATA_JOB_NAME}"
           echo "FEATURE_ENGINEERING_JOB_NAME=${FEATURE_ENGINEERING_JOB_NAME}"

           # NOTE: checking db
           bq ls $PRJECT_ID:bike_streaming
           # tableId        Type    Labels      Time Partitioning       Clustered Fields
           # ------------------ ------- -------- ------------------------- ---------------------
           # raw_events         TABLE            DAY (field: event_date)
           # station_features   TABLE            DAY (field: event_date)   city_id, station_id

           bq query --use_legacy_sql=false "SELECT * FROM \`$RAW_EVENT_TABLE\` ORDER BY ts_iso desc  LIMIT 10;"
           bq query --use_legacy_sql=false "SELECT * FROM \`$FEATURES_TABLE\` ORDER BY as_of DESC;"

           # NOTE: checking pubsub
           gcloud pubsub topics list --project="$PROJECT_ID"
           gcloud pubsub subscriptions list --project="$PROJECT_ID"

           # NOTE: checking SA permissions
           gcloud iam service-accounts list --project=mlops-project-abacabb
           gcloud projects get-iam-policy "${PROJECT_ID}" \
                  --flatten="bindings[].members" \
                  --filter="bindings.members:serviceAccount:${SA}" \
                  --format="table(bindings.role)"

           # NOTE: add required permissions
           gcloud projects add-iam-policy-binding "${PROJECT_ID}" \
                  --member="serviceAccount:${SA}" \
                  --role="roles/bigquery.jobUser"


           # NOTE: re start apache beam job INJECT_SENSOR_DATA_JOB_NAME
           python scripts/create_apache_beam_stream_pubsub_to_raw_events.py &

           # NOTE: checking dataflow
           gcloud dataflow jobs list --region "$REGION"
           JOB_ID=2025-11-15_16_52_15-3602385465536419454
           gcloud dataflow jobs describe "$JOB_ID" --region="$REGION"
         #+end_src

     * run locally (read and save on db on cloud)

       #+begin_src sh
         # NOTE: run
         # 1) as test (read from sjon file)
         python scripts/run_apache_beam_batch_raw_events_to_features.py \
                --runner=test --run-timestamp=2025-12-13

         ls data/*.json

         # 2) locally. run job locally but read and write on db on goodle cloud
         # also save csv loaded data for debug
         python scripts/run_apache_beam_batch_raw_events_to_features.py \
                --runner=DirectRunner

         ls data/*.csv

         # 3) run job on datacloud
         python scripts/run_apache_beam_batch_raw_events_to_features.py \
                --runner=DataflowRunner


         # NOTE: this creates 3 files per run on the staging buckets:
         gsutil ls -lh "${BUCKET}/bike_stream/staging/**"
         gsutil ls -lh "${BUCKET}/bike_stream/staging/beamapp-leandrofernandes-1222041732-758158-j24fym5s.1766377052.758245/**"

         # 1) pipeline.pb: protobuffer file representing the pipeline. (contains the serialized Beam pipeline definition / job graph)
         # 2) submission_environment_dependencies.txt: with python version, python packages. (snapshot of the environment used to submit)
         # 3) workflow.tar.gz: the code repo zipped. staged source distribution of your code (the stuff built from setup.py)
       #+end_src

     * Optional: delete audit/debug files under run/ after 14 days

       * create the file: lifecycle.json (age on days):

         #+begin_src json
           {
               "rule": [
                   {
                       "action": {"type": "Delete"},
                       "condition": {
                           "age": 7,
                           "matchesPrefix": ["run/"]
                       }
                   }
               ]
           }
         #+end_src

       * Apply it to the bucket:

         #+begin_src sh
           gsutil lifecycle set lifecycle.json $BUCKET
         #+end_src

     * setup cloud scheduler

       * set docker images

         #+begin_src sh
           # NOTE: setup docke registry
           gcloud artifacts repositories create "${REGISTRY}" \
                  --repository-format=docker \
                  --location="${REGION}" \
                  --project="${PROJECT_ID}" \
                  --description="Images for dataflow jobs"


           # 1) Inspect all registry in the project
           gcloud artifacts repositories list \
                  --project="$PROJECT_ID" \
                  --filter='format=DOCKER' \
                  --format="table(name,location,createTime)"

           # NOTE: build the image
           # 1) google cloud
           gcloud builds submit --tag "${IMAGE}" .

           # 2) local
           docker build -t ${IMAGE} .
           docker images

           # 3) verify code inside docker image
           docker run --rm --entrypoint ls ${IMAGE} -la /app
           docker run --rm --entrypoint ls ${IMAGE} -la /app/common_pipelines

           # 4) check pyton path
           docker run --rm --entrypoint python ${IMAGE} -c "import sys; print(sys.path)"
           docker run --rm --entrypoint ls ${IMAGE} -la /app/data/*
           docker run --rm --entrypoint cat ${IMAGE}  /app/data/features.json

           docker run --rm -it \
                  -e PROJECT_ID="$PROJECT_ID" \
                  -e REGION="$REGION" \
                  -e RAW_EVENT_TABLE="$RAW_EVENT_TABLE" \
                  -e FEATURES_TABLE="$FEATURES_TABLE" \
                  -e BUCKET="$BUCKET" \
                  --entrypoint /bin/bash \
                  ${IMAGE}

           # NOTE: push the image
           docker push ${IMAGE}

           # 2) list all docerk images
           gcloud artifacts docker images list \
                  "$REGION-docker.pkg.dev/$PROJECT_ID/$REGISTRY" \
                  --format="table(package,updateTime)"


         #+end_src

       * set cloud job

         #+begin_src sh
           # NOTE: create the cloud scheduler
           # 1) create cloud job. This will run quickly and just start the dataflow job
           # defined by the script: scripts/run_apache_beam_batch_raw_events_to_features.py
           gcloud run jobs create update-station-features-launcher \
                  --image="${IMAGE}" \
                  --region="${REGION}" \
                  --project="${PROJECT_ID}" \
                  --service-account="${SA}" \
                  --set-env-vars="PROJECT_ID=${PROJECT_ID},REGION=${REGION},RAW_EVENT_TABLE=${RAW_EVENT_TABLE},FEATURES_TABLE=${FEATURES_TABLE},BUCKET=${BUCKET}"

           # 1.1.) list all jobs
           gcloud run jobs list --project="$PROJECT_ID" --region="$REGION"


           # 1.2) check the service account
           gcloud run jobs describe update-station-features-launcher \
                    --region="${REGION}" \
                    --project="${PROJECT_ID}" \
                    --format="value(spec.template.spec.template.spec.serviceAccountName)"

           # 1.3) test an execution
           gcloud run jobs execute update-station-features-launcher


           # NOTE: review and add notes here regards permissions and authentications.
           # When running cloud jobs it will run with the permissions of the service
           # accounts passed on creation time and it will be able to authenticate using
           # this service account for runn the dataflow jobs. NO need to worry about
           # athentictaion only when running form docker from local laptop

           # NOTE: start dataflow job from runner docker conatiner
           echo $GOOGLE_APPLICATION_CREDENTIALS
           docker run --rm \
             -v "$GOOGLE_APPLICATION_CREDENTIALS:/tmp/key.json:ro" \
             -e GOOGLE_APPLICATION_CREDENTIALS="/tmp/key.json" \
             -e PROJECT_ID="$PROJECT_ID" -e REGION="$REGION" \
             -e RAW_EVENT_TABLE="$RAW_EVENT_TABLE" -e FEATURES_TABLE="$FEATURES_TABLE" \
             -e BUCKET="$BUCKET" \
             "${IMAGE}" --runner=DataflowRunner

           # inspect clud jobs
           gcloud run jobs list --project="$PROJECT_ID" --region="$REGION"

           # inspect dataflow jobs
           gcloud dataflow jobs list --region="$REGION" --project="$PROJECT_ID"
         #+end_src

       * set the scheduler

           #+begin_src sh
             # NOTE: schedule every 10 minutes
             gcloud scheduler jobs create http update-station-features-job \
                    --schedule="*/10 * * * *" \
                    --http-method=POST \
                    --uri=${URI} \
                    --message-body="{}" \
                    --time-zone="America/Los_Angeles" \
                    --location=${REGION}

             gcloud run jobs create update-station-features-launcher \
                    --image="${IMAGE}" \
                    --region="${REGION}" \
                    --project="${PROJECT_ID}" \
                    --service-account="${LAUNCHER_SA}" \
                    --set-env-vars="PROJECT_ID=${PROJECT_ID},REGION=${REGION},RAW_EVENT_TABLE=${RAW_EVENT_TABLE},FEATURES_TABLE=${FEATURES_TABLE},BUCKET=${BUCKET}"


             # NOTE: inspect
             gcloud scheduler jobs list --location=$REGION
             gcloud scheduler jobs describe update-station-features-job --location=$REGION

             # NOTE: test solution

             # 0) make sure dataflow is running
             gcloud dataflow jobs list --region "$REGION"
             # 1) inspect the state of features table
             bq query --use_legacy_sql=false "SELECT * FROM \`$FEATURES_TABLE\` ORDER BY as_of DESC;"
             # 2) run sensor
             python scripts/sensors.py
             # 3): manually trigger the job
             gcloud scheduler jobs run update-station-features-job --location=${REGION}


           #+end_src



         

** How to test

    #+begin_src sh
      # NOTE: run feature engineering pipeline with fake data
      # Do NOT insert in FEATURES_TABLE
      python tests/test_feature_logic.py
    #+end_src

    
